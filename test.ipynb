{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "with open(\"./oai-config/OAI_API_KEY\") as f:\n",
    "    os.environ['OPENAI_API_KEY'] = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pyautogen/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'api_key': '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'}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import autogen\n",
    "\n",
    "endpoint_list = autogen.config_list_openai_aoai()\n",
    "endpoint_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'model': 'gpt-4',\n",
       "  'api_key': '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'},\n",
       " {'model': 'gpt-3.5-turbo-0613',\n",
       "  'api_key': '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'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_list = autogen.config_list_from_json(\n",
    "    env_or_file=\"OAI_CONFIG_LIST\",\n",
    "    filter_dict={\n",
    "        \"model\": {\n",
    "            \"gpt-3.5-turbo\",\n",
    "            \"gpt-3.5-turbo-16k\",\n",
    "            \"gpt-3.5-turbo-0301\",\n",
    "            \"chatgpt-35-turbo-0301\",\n",
    "            \"gpt-35-turbo-v0301\",\n",
    "            \"gpt\",\n",
    "        },\n",
    "    },\n",
    ")\n",
    "config_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "\n",
    "First, we load the humaneval dataset. The dataset contains 164 examples. We use the first 20 for tuning the generation hyperparameters and the remaining for evaluation. In each example, the \"prompt\" is the prompt string for eliciting the code generation (renamed into \"definition\"), \"test\" is the Python code for unit test for the example, and \"entry_point\" is the function name to be tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "seed = 40\n",
    "data = datasets.load_dataset(\"openai_humaneval\")[\"test\"].shuffle(seed=seed)\n",
    "n_tune_data = 20\n",
    "tune_data = [\n",
    "    {\n",
    "        \"definition\": data[x][\"prompt\"],\n",
    "        \"test\": data[x][\"test\"],\n",
    "        \"entry_point\": data[x][\"entry_point\"],\n",
    "    }\n",
    "    for x in range(n_tune_data)\n",
    "]\n",
    "test_data = [\n",
    "    {\n",
    "        \"definition\": data[x][\"prompt\"],\n",
    "        \"test\": data[x][\"test\"],\n",
    "        \"entry_point\": data[x][\"entry_point\"],\n",
    "    }\n",
    "    for x in range(n_tune_data, len(data))\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "def sorted_list_sum(lst):\n",
      "    \"\"\"Write a function that accepts a list of strings as a parameter,\n",
      "    deletes the strings that have odd lengths from it,\n",
      "    and returns the resulted list with a sorted order,\n",
      "    The list is always a list of strings and never an array of numbers,\n",
      "    and it may contain duplicates.\n",
      "    The order of the list should be ascending by length of each word, and you\n",
      "    should return the list sorted by that rule.\n",
      "    If two words have the same length, sort the list alphabetically.\n",
      "    The function should return a list of strings in sorted order.\n",
      "    You may assume that all words will have the same length.\n",
      "    For example:\n",
      "    assert list_sort([\"aa\", \"a\", \"aaa\"]) => [\"aa\"]\n",
      "    assert list_sort([\"ab\", \"a\", \"aaa\", \"cd\"]) => [\"ab\", \"cd\"]\n",
      "    \"\"\"\n",
      "\n",
      "def check(candidate):\n",
      "\n",
      "    # Check some simple cases\n",
      "    assert candidate([\"aa\", \"a\", \"aaa\"]) == [\"aa\"]\n",
      "    assert candidate([\"school\", \"AI\", \"asdf\", \"b\"]) == [\"AI\", \"asdf\", \"school\"]\n",
      "    assert candidate([\"d\", \"b\", \"c\", \"a\"]) == []\n",
      "    assert candidate([\"d\", \"dcba\", \"abcd\", \"a\"]) == [\"abcd\", \"dcba\"]\n",
      "\n",
      "    # Check some edge cases that are easy to work out by hand.\n",
      "    assert candidate([\"AI\", \"ai\", \"au\"]) == [\"AI\", \"ai\", \"au\"]\n",
      "    assert candidate([\"a\", \"b\", \"b\", \"c\", \"c\", \"a\"]) == []\n",
      "    assert candidate(['aaaa', 'bbbb', 'dd', 'cc']) == [\"cc\", \"dd\", \"aaaa\", \"bbbb\"]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tune_data[1][\"definition\"])\n",
    "print(tune_data[1][\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyautogen~=0.1.0 in /home/ubuntu/anaconda3/envs/pyautogen/lib/python3.10/site-packages (from pyautogen[blendsearch]~=0.1.0) (0.1.14)\n",
      "Requirement already satisfied: diskcache in /home/ubuntu/anaconda3/envs/pyautogen/lib/python3.10/site-packages (from pyautogen~=0.1.0->pyautogen[blendsearch]~=0.1.0) (5.6.3)\n",
      "Requirement already satisfied: flaml in /home/ubuntu/anaconda3/envs/pyautogen/lib/python3.10/site-packages (from pyautogen~=0.1.0->pyautogen[blendsearch]~=0.1.0) (2.1.1)\n",
      "Requirement already satisfied: openai<1 in /home/ubuntu/anaconda3/envs/pyautogen/lib/python3.10/site-packages (from pyautogen~=0.1.0->pyautogen[blendsearch]~=0.1.0) (0.28.1)\n",
      "Requirement already satisfied: python-dotenv in /home/ubuntu/anaconda3/envs/pyautogen/lib/python3.10/site-packages (from pyautogen~=0.1.0->pyautogen[blendsearch]~=0.1.0) (1.0.0)\n",
      "Requirement already satisfied: termcolor in /home/ubuntu/anaconda3/envs/pyautogen/lib/python3.10/site-packages (from pyautogen~=0.1.0->pyautogen[blendsearch]~=0.1.0) (2.4.0)\n",
      "Requirement already satisfied: requests>=2.20 in /home/ubuntu/anaconda3/envs/pyautogen/lib/python3.10/site-packages (from openai<1->pyautogen~=0.1.0->pyautogen[blendsearch]~=0.1.0) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /home/ubuntu/anaconda3/envs/pyautogen/lib/python3.10/site-packages (from openai<1->pyautogen~=0.1.0->pyautogen[blendsearch]~=0.1.0) (4.66.1)\n",
      "Requirement already satisfied: aiohttp in /home/ubuntu/anaconda3/envs/pyautogen/lib/python3.10/site-packages (from openai<1->pyautogen~=0.1.0->pyautogen[blendsearch]~=0.1.0) (3.9.1)\n",
      "Requirement already satisfied: NumPy>=1.17.0rc1 in /home/ubuntu/anaconda3/envs/pyautogen/lib/python3.10/site-packages (from flaml->pyautogen~=0.1.0->pyautogen[blendsearch]~=0.1.0) (1.26.2)\n",
      "Requirement already satisfied: optuna==2.8.0 in /home/ubuntu/anaconda3/envs/pyautogen/lib/python3.10/site-packages (from flaml[blendsearch]; extra == \"blendsearch\"->pyautogen[blendsearch]~=0.1.0) (2.8.0)\n",
      "Requirement already satisfied: packaging in /home/ubuntu/anaconda3/envs/pyautogen/lib/python3.10/site-packages (from flaml[blendsearch]; extra == \"blendsearch\"->pyautogen[blendsearch]~=0.1.0) (23.2)\n",
      "Requirement already satisfied: alembic in /home/ubuntu/anaconda3/envs/pyautogen/lib/python3.10/site-packages (from optuna==2.8.0->flaml[blendsearch]; extra == \"blendsearch\"->pyautogen[blendsearch]~=0.1.0) (1.13.0)\n",
      "Requirement already satisfied: cliff in /home/ubuntu/anaconda3/envs/pyautogen/lib/python3.10/site-packages (from optuna==2.8.0->flaml[blendsearch]; extra == \"blendsearch\"->pyautogen[blendsearch]~=0.1.0) (4.4.0)\n",
      "Requirement already satisfied: cmaes>=0.8.2 in /home/ubuntu/anaconda3/envs/pyautogen/lib/python3.10/site-packages (from optuna==2.8.0->flaml[blendsearch]; extra == \"blendsearch\"->pyautogen[blendsearch]~=0.1.0) (0.10.0)\n",
      "Requirement already satisfied: colorlog in /home/ubuntu/anaconda3/envs/pyautogen/lib/python3.10/site-packages (from optuna==2.8.0->flaml[blendsearch]; extra == \"blendsearch\"->pyautogen[blendsearch]~=0.1.0) (6.8.0)\n",
      "Requirement already satisfied: scipy!=1.4.0 in /home/ubuntu/anaconda3/envs/pyautogen/lib/python3.10/site-packages (from optuna==2.8.0->flaml[blendsearch]; extra == \"blendsearch\"->pyautogen[blendsearch]~=0.1.0) (1.11.4)\n",
      "Requirement already satisfied: sqlalchemy>=1.1.0 in /home/ubuntu/anaconda3/envs/pyautogen/lib/python3.10/site-packages (from optuna==2.8.0->flaml[blendsearch]; extra == \"blendsearch\"->pyautogen[blendsearch]~=0.1.0) (1.4.50)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/anaconda3/envs/pyautogen/lib/python3.10/site-packages (from requests>=2.20->openai<1->pyautogen~=0.1.0->pyautogen[blendsearch]~=0.1.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/anaconda3/envs/pyautogen/lib/python3.10/site-packages (from requests>=2.20->openai<1->pyautogen~=0.1.0->pyautogen[blendsearch]~=0.1.0) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ubuntu/anaconda3/envs/pyautogen/lib/python3.10/site-packages (from requests>=2.20->openai<1->pyautogen~=0.1.0->pyautogen[blendsearch]~=0.1.0) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/anaconda3/envs/pyautogen/lib/python3.10/site-packages (from requests>=2.20->openai<1->pyautogen~=0.1.0->pyautogen[blendsearch]~=0.1.0) (2023.11.17)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ubuntu/anaconda3/envs/pyautogen/lib/python3.10/site-packages (from aiohttp->openai<1->pyautogen~=0.1.0->pyautogen[blendsearch]~=0.1.0) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ubuntu/anaconda3/envs/pyautogen/lib/python3.10/site-packages (from aiohttp->openai<1->pyautogen~=0.1.0->pyautogen[blendsearch]~=0.1.0) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ubuntu/anaconda3/envs/pyautogen/lib/python3.10/site-packages (from aiohttp->openai<1->pyautogen~=0.1.0->pyautogen[blendsearch]~=0.1.0) (1.9.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ubuntu/anaconda3/envs/pyautogen/lib/python3.10/site-packages (from aiohttp->openai<1->pyautogen~=0.1.0->pyautogen[blendsearch]~=0.1.0) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ubuntu/anaconda3/envs/pyautogen/lib/python3.10/site-packages (from aiohttp->openai<1->pyautogen~=0.1.0->pyautogen[blendsearch]~=0.1.0) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/ubuntu/anaconda3/envs/pyautogen/lib/python3.10/site-packages (from aiohttp->openai<1->pyautogen~=0.1.0->pyautogen[blendsearch]~=0.1.0) (4.0.3)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/ubuntu/anaconda3/envs/pyautogen/lib/python3.10/site-packages (from sqlalchemy>=1.1.0->optuna==2.8.0->flaml[blendsearch]; extra == \"blendsearch\"->pyautogen[blendsearch]~=0.1.0) (3.0.1)\n",
      "Requirement already satisfied: Mako in /home/ubuntu/anaconda3/envs/pyautogen/lib/python3.10/site-packages (from alembic->optuna==2.8.0->flaml[blendsearch]; extra == \"blendsearch\"->pyautogen[blendsearch]~=0.1.0) (1.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4 in /home/ubuntu/anaconda3/envs/pyautogen/lib/python3.10/site-packages (from alembic->optuna==2.8.0->flaml[blendsearch]; extra == \"blendsearch\"->pyautogen[blendsearch]~=0.1.0) (4.8.0)\n",
      "Requirement already satisfied: PrettyTable>=0.7.2 in /home/ubuntu/anaconda3/envs/pyautogen/lib/python3.10/site-packages (from cliff->optuna==2.8.0->flaml[blendsearch]; extra == \"blendsearch\"->pyautogen[blendsearch]~=0.1.0) (3.9.0)\n",
      "Requirement already satisfied: PyYAML>=3.12 in /home/ubuntu/anaconda3/envs/pyautogen/lib/python3.10/site-packages (from cliff->optuna==2.8.0->flaml[blendsearch]; extra == \"blendsearch\"->pyautogen[blendsearch]~=0.1.0) (6.0.1)\n",
      "Requirement already satisfied: autopage>=0.4.0 in /home/ubuntu/anaconda3/envs/pyautogen/lib/python3.10/site-packages (from cliff->optuna==2.8.0->flaml[blendsearch]; extra == \"blendsearch\"->pyautogen[blendsearch]~=0.1.0) (0.5.2)\n",
      "Requirement already satisfied: cmd2>=1.0.0 in /home/ubuntu/anaconda3/envs/pyautogen/lib/python3.10/site-packages (from cliff->optuna==2.8.0->flaml[blendsearch]; extra == \"blendsearch\"->pyautogen[blendsearch]~=0.1.0) (2.4.3)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /home/ubuntu/anaconda3/envs/pyautogen/lib/python3.10/site-packages (from cliff->optuna==2.8.0->flaml[blendsearch]; extra == \"blendsearch\"->pyautogen[blendsearch]~=0.1.0) (6.11.0)\n",
      "Requirement already satisfied: stevedore>=2.0.1 in /home/ubuntu/anaconda3/envs/pyautogen/lib/python3.10/site-packages (from cliff->optuna==2.8.0->flaml[blendsearch]; extra == \"blendsearch\"->pyautogen[blendsearch]~=0.1.0) (5.1.0)\n",
      "Requirement already satisfied: pyperclip>=1.6 in /home/ubuntu/anaconda3/envs/pyautogen/lib/python3.10/site-packages (from cmd2>=1.0.0->cliff->optuna==2.8.0->flaml[blendsearch]; extra == \"blendsearch\"->pyautogen[blendsearch]~=0.1.0) (1.8.2)\n",
      "Requirement already satisfied: wcwidth>=0.1.7 in /home/ubuntu/anaconda3/envs/pyautogen/lib/python3.10/site-packages (from cmd2>=1.0.0->cliff->optuna==2.8.0->flaml[blendsearch]; extra == \"blendsearch\"->pyautogen[blendsearch]~=0.1.0) (0.2.12)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ubuntu/anaconda3/envs/pyautogen/lib/python3.10/site-packages (from importlib-metadata>=4.4->cliff->optuna==2.8.0->flaml[blendsearch]; extra == \"blendsearch\"->pyautogen[blendsearch]~=0.1.0) (3.17.0)\n",
      "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /home/ubuntu/anaconda3/envs/pyautogen/lib/python3.10/site-packages (from stevedore>=2.0.1->cliff->optuna==2.8.0->flaml[blendsearch]; extra == \"blendsearch\"->pyautogen[blendsearch]~=0.1.0) (6.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /home/ubuntu/anaconda3/envs/pyautogen/lib/python3.10/site-packages (from Mako->alembic->optuna==2.8.0->flaml[blendsearch]; extra == \"blendsearch\"->pyautogen[blendsearch]~=0.1.0) (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install \"pyautogen[blendsearch]~=0.1.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "eval_with_generated_assertions = partial(\n",
    "    autogen.code_utils.eval_function_completions,\n",
    "    assertions=partial(autogen.code_utils.generate_assertions, config_list=config_list),\n",
    "    use_docker=False,\n",
    "    # Please set use_docker=True if docker is available to run the generated code.\n",
    "    # Using docker is safer than running the generated code directly.\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "autogen.Completion.set_cache(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:flaml.tune.searcher.blendsearch:No low-cost partial config given to the search algorithm. For cost-frugal search, consider providing low-cost values for cost-related hps via 'low_cost_partial_config'. More info can be found at https://microsoft.github.io/FLAML/docs/FAQ#about-low_cost_partial_config-in-tune\n",
      "\u001b[32m[I 2023-12-04 10:05:16,131]\u001b[0m A new study created in memory with name: optuna\u001b[0m\n",
      "INFO:flaml.tune.searcher.blendsearch:No low-cost partial config given to the search algorithm. For cost-frugal search, consider providing low-cost values for cost-related hps via 'low_cost_partial_config'. More info can be found at https://microsoft.github.io/FLAML/docs/FAQ#about-low_cost_partial_config-in-tune\n",
      "\u001b[32m[I 2023-12-04 10:05:16,135]\u001b[0m A new study created in memory with name: optuna\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.tune.tune: 12-04 10:05:16] {805} INFO - trial 1 config: {'prompt': 1, 'stop': 0, 'allow_format_str_template': True, 'subspace': {'model': 'text-ada-001', 'max_tokens': 148, 'temperature_or_top_p': {'top_p': 0.755486898036596}, 'n': 27}}\n",
      "[flaml.tune.tune: 12-04 10:05:29] {197} INFO - result: {'index_selected': 26.0, 'succeed_assertions': 0.0, 'success': 0.0, 'gen_cost': 0.0038099999999999996, 'assertions': \"I'm sorry, but I can't provide the assertions without the actual function definition and examples in the docstring you mentioned. Please provide the function signature and docstring so I can help you better.\", 'total_cost': 0.010623200000000001, 'cost': 0.010623200000000001, 'inference_cost': 0.00025844, 'training_iteration': 0, 'config': {'prompt': 1, 'stop': 0, 'allow_format_str_template': True, 'subspace': {'model': 'text-ada-001', 'max_tokens': 148, 'temperature_or_top_p': {'top_p': 0.755486898036596}, 'n': 27}}, 'config/prompt': 1, 'config/stop': 0, 'config/allow_format_str_template': True, 'config/subspace': {'model': 'text-ada-001', 'max_tokens': 148, 'temperature_or_top_p': {'top_p': 0.755486898036596}, 'n': 27}, 'experiment_tag': 'exp', 'time_total_s': 13.788267612457275}\n",
      "[flaml.tune.tune: 12-04 10:05:29] {805} INFO - trial 2 config: {'prompt': 1, 'stop': 0, 'allow_format_str_template': True, 'subspace': {'model': 'text-babbage-001', 'max_tokens': 148, 'temperature_or_top_p': {'top_p': 0.755486898036596}, 'n': 27}}\n",
      "[flaml.tune.tune: 12-04 10:06:03] {197} INFO - result: {'index_selected': 26.0, 'succeed_assertions': 0.0, 'success': 0.0, 'gen_cost': 0.0038099999999999996, 'assertions': \"I'm sorry, but I can't provide the assertions without the actual function definition and examples in the docstring you mentioned. Please provide the function signature and docstring so I can help you better.\", 'total_cost': 0.0263002, 'cost': 0.015677, 'inference_cost': 0.00078385, 'training_iteration': 0, 'config': {'prompt': 1, 'stop': 0, 'allow_format_str_template': True, 'subspace': {'model': 'text-babbage-001', 'max_tokens': 148, 'temperature_or_top_p': {'top_p': 0.755486898036596}, 'n': 27}}, 'config/prompt': 1, 'config/stop': 0, 'config/allow_format_str_template': True, 'config/subspace': {'model': 'text-babbage-001', 'max_tokens': 148, 'temperature_or_top_p': {'top_p': 0.755486898036596}, 'n': 27}, 'experiment_tag': 'exp', 'time_total_s': 33.3153555393219}\n",
      "[flaml.tune.tune: 12-04 10:06:03] {805} INFO - trial 3 config: {'prompt': 1, 'stop': 0, 'allow_format_str_template': True, 'subspace': {'model': 'text-davinci-003', 'max_tokens': 148, 'temperature_or_top_p': {'top_p': 0.755486898036596}, 'n': 27}}\n",
      "[flaml.tune.tune: 12-04 10:07:13] {197} INFO - result: {'index_selected': 26.0, 'succeed_assertions': 0.0, 'success': 0.45, 'gen_cost': 0.0038099999999999996, 'assertions': \"I'm sorry, but I can't provide the assertions without the actual function definition and examples in the docstring you mentioned. Please provide the function signature and docstring so I can help you better.\", 'total_cost': 0.7934202, 'cost': 0.76712, 'inference_cost': 0.037582, 'training_iteration': 0, 'config': {'prompt': 1, 'stop': 0, 'allow_format_str_template': True, 'subspace': {'model': 'text-davinci-003', 'max_tokens': 148, 'temperature_or_top_p': {'top_p': 0.755486898036596}, 'n': 27}}, 'config/prompt': 1, 'config/stop': 0, 'config/allow_format_str_template': True, 'config/subspace': {'model': 'text-davinci-003', 'max_tokens': 148, 'temperature_or_top_p': {'top_p': 0.755486898036596}, 'n': 27}, 'experiment_tag': 'exp', 'time_total_s': 70.30482292175293}\n",
      "[flaml.tune.tune: 12-04 10:07:13] {805} INFO - trial 4 config: {'prompt': 1, 'stop': 0, 'allow_format_str_template': True, 'subspace': {'model': 'gpt-3.5-turbo', 'max_tokens': 148, 'temperature_or_top_p': {'top_p': 0.755486898036596}, 'n': 27}}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/git/DBMA/test.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bec2-44-203-122-17.compute-1.amazonaws.com/home/ubuntu/git/DBMA/test.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m config, analysis \u001b[39m=\u001b[39m autogen\u001b[39m.\u001b[39;49mCompletion\u001b[39m.\u001b[39;49mtune(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bec2-44-203-122-17.compute-1.amazonaws.com/home/ubuntu/git/DBMA/test.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     data\u001b[39m=\u001b[39;49mtune_data,  \u001b[39m# the data for tuning\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bec2-44-203-122-17.compute-1.amazonaws.com/home/ubuntu/git/DBMA/test.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     metric\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39msuccess\u001b[39;49m\u001b[39m\"\u001b[39;49m,  \u001b[39m# the metric to optimize\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bec2-44-203-122-17.compute-1.amazonaws.com/home/ubuntu/git/DBMA/test.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     mode\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmax\u001b[39;49m\u001b[39m\"\u001b[39;49m,  \u001b[39m# the optimization mode\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bec2-44-203-122-17.compute-1.amazonaws.com/home/ubuntu/git/DBMA/test.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     eval_func\u001b[39m=\u001b[39;49meval_with_generated_assertions,  \u001b[39m# the evaluation function to return the success metrics\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bec2-44-203-122-17.compute-1.amazonaws.com/home/ubuntu/git/DBMA/test.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m# log_file_name=\"logs/humaneval.log\",  # the log file name\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bec2-44-203-122-17.compute-1.amazonaws.com/home/ubuntu/git/DBMA/test.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     inference_budget\u001b[39m=\u001b[39;49m\u001b[39m0.05\u001b[39;49m,  \u001b[39m# the inference budget (dollar per instance)\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bec2-44-203-122-17.compute-1.amazonaws.com/home/ubuntu/git/DBMA/test.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     optimization_budget\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,  \u001b[39m# the optimization budget (dollar in total)\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bec2-44-203-122-17.compute-1.amazonaws.com/home/ubuntu/git/DBMA/test.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39m# num_samples can further limit the number of trials for different hyperparameter configurations;\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bec2-44-203-122-17.compute-1.amazonaws.com/home/ubuntu/git/DBMA/test.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39m# -1 means decided by the optimization budget only\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bec2-44-203-122-17.compute-1.amazonaws.com/home/ubuntu/git/DBMA/test.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     num_samples\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bec2-44-203-122-17.compute-1.amazonaws.com/home/ubuntu/git/DBMA/test.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     prompt\u001b[39m=\u001b[39;49m[\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bec2-44-203-122-17.compute-1.amazonaws.com/home/ubuntu/git/DBMA/test.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39m{definition}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bec2-44-203-122-17.compute-1.amazonaws.com/home/ubuntu/git/DBMA/test.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39m# Python 3\u001b[39;49m\u001b[39m{definition}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bec2-44-203-122-17.compute-1.amazonaws.com/home/ubuntu/git/DBMA/test.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mComplete the following Python function:\u001b[39;49m\u001b[39m{definition}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bec2-44-203-122-17.compute-1.amazonaws.com/home/ubuntu/git/DBMA/test.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     ],  \u001b[39m# the prompt templates to choose from\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bec2-44-203-122-17.compute-1.amazonaws.com/home/ubuntu/git/DBMA/test.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m     stop\u001b[39m=\u001b[39;49m[[\u001b[39m\"\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39mclass\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39mdef\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39mif\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39mprint\u001b[39;49m\u001b[39m\"\u001b[39;49m], \u001b[39mNone\u001b[39;49;00m],  \u001b[39m# the stop sequences\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bec2-44-203-122-17.compute-1.amazonaws.com/home/ubuntu/git/DBMA/test.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m     config_list\u001b[39m=\u001b[39;49mendpoint_list,  \u001b[39m# optional: a list of endpoints to use\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bec2-44-203-122-17.compute-1.amazonaws.com/home/ubuntu/git/DBMA/test.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m     allow_format_str_template\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,  \u001b[39m# whether to allow format string template\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bec2-44-203-122-17.compute-1.amazonaws.com/home/ubuntu/git/DBMA/test.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/pyautogen/lib/python3.10/site-packages/autogen/oai/completion.py:691\u001b[0m, in \u001b[0;36mCompletion.tune\u001b[0;34m(cls, data, metric, mode, eval_func, log_file_name, inference_budget, optimization_budget, num_samples, logging_level, **config)\u001b[0m\n\u001b[1;32m    689\u001b[0m logger\u001b[39m.\u001b[39msetLevel(logging_level)\n\u001b[1;32m    690\u001b[0m \u001b[39mwith\u001b[39;00m diskcache\u001b[39m.\u001b[39mCache(\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mcache_path) \u001b[39mas\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_cache:\n\u001b[0;32m--> 691\u001b[0m     analysis \u001b[39m=\u001b[39m tune\u001b[39m.\u001b[39;49mrun(\n\u001b[1;32m    692\u001b[0m         \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_eval,\n\u001b[1;32m    693\u001b[0m         search_alg\u001b[39m=\u001b[39;49msearch_alg,\n\u001b[1;32m    694\u001b[0m         num_samples\u001b[39m=\u001b[39;49mnum_samples,\n\u001b[1;32m    695\u001b[0m         log_file_name\u001b[39m=\u001b[39;49mlog_file_name,\n\u001b[1;32m    696\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m,\n\u001b[1;32m    697\u001b[0m     )\n\u001b[1;32m    698\u001b[0m config \u001b[39m=\u001b[39m analysis\u001b[39m.\u001b[39mbest_config\n\u001b[1;32m    699\u001b[0m params \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_get_params_for_create(config)\n",
      "File \u001b[0;32m~/anaconda3/envs/pyautogen/lib/python3.10/site-packages/flaml/tune/tune.py:808\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(evaluation_function, config, low_cost_partial_config, cat_hp_cost, metric, mode, time_budget_s, points_to_evaluate, evaluated_rewards, resource_attr, min_resource, max_resource, reduction_factor, scheduler, search_alg, verbose, local_dir, num_samples, resources_per_trial, config_constraints, metric_constraints, max_failure, use_ray, use_spark, use_incumbent_result_in_evaluation, log_file_name, lexico_objectives, force_cancel, n_concurrent_trials, **ray_args)\u001b[0m\n\u001b[1;32m    806\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    807\u001b[0m \u001b[39mwith\u001b[39;00m PySparkOvertimeMonitor(time_start, time_budget_s, force_cancel):\n\u001b[0;32m--> 808\u001b[0m     result \u001b[39m=\u001b[39m evaluation_function(trial_to_run\u001b[39m.\u001b[39;49mconfig)\n\u001b[1;32m    809\u001b[0m \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    810\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(result, \u001b[39mdict\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/pyautogen/lib/python3.10/site-packages/autogen/oai/completion.py:407\u001b[0m, in \u001b[0;36mCompletion._eval\u001b[0;34m(cls, config, prune, eval_only)\u001b[0m\n\u001b[1;32m    405\u001b[0m logger\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mnum_completions=\u001b[39m\u001b[39m{\u001b[39;00mnum_completions\u001b[39m}\u001b[39;00m\u001b[39m, data instance=\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    406\u001b[0m data_i \u001b[39m=\u001b[39m data[i]\n\u001b[0;32m--> 407\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mcreate(data_i, raise_on_ratelimit_or_timeout\u001b[39m=\u001b[39;49meval_only, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams)\n\u001b[1;32m    408\u001b[0m \u001b[39mif\u001b[39;00m response \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:  \u001b[39m# rate limit/timeout error, treat as invalid\u001b[39;00m\n\u001b[1;32m    409\u001b[0m     \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_update_invalid_n(prune, region_key, max_tokens, num_completions)\n",
      "File \u001b[0;32m~/anaconda3/envs/pyautogen/lib/python3.10/site-packages/autogen/oai/completion.py:803\u001b[0m, in \u001b[0;36mCompletion.create\u001b[0;34m(cls, context, use_cache, config_list, filter_func, raise_on_ratelimit_or_timeout, allow_format_str_template, **config)\u001b[0m\n\u001b[1;32m    801\u001b[0m     base_config[\u001b[39m\"\u001b[39m\u001b[39mmax_retry_period\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    802\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 803\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m    804\u001b[0m         context,\n\u001b[1;32m    805\u001b[0m         use_cache,\n\u001b[1;32m    806\u001b[0m         raise_on_ratelimit_or_timeout\u001b[39m=\u001b[39;49mi \u001b[39m<\u001b[39;49m last \u001b[39mor\u001b[39;49;00m raise_on_ratelimit_or_timeout,\n\u001b[1;32m    807\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mbase_config,\n\u001b[1;32m    808\u001b[0m     )\n\u001b[1;32m    809\u001b[0m     \u001b[39mif\u001b[39;00m response \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n\u001b[1;32m    810\u001b[0m         \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/anaconda3/envs/pyautogen/lib/python3.10/site-packages/autogen/oai/completion.py:834\u001b[0m, in \u001b[0;36mCompletion.create\u001b[0;34m(cls, context, use_cache, config_list, filter_func, raise_on_ratelimit_or_timeout, allow_format_str_template, **config)\u001b[0m\n\u001b[1;32m    832\u001b[0m \u001b[39mwith\u001b[39;00m diskcache\u001b[39m.\u001b[39mCache(\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mcache_path) \u001b[39mas\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_cache:\n\u001b[1;32m    833\u001b[0m     \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mset_cache(seed)\n\u001b[0;32m--> 834\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_get_response(params, raise_on_ratelimit_or_timeout\u001b[39m=\u001b[39;49mraise_on_ratelimit_or_timeout)\n",
      "File \u001b[0;32m~/anaconda3/envs/pyautogen/lib/python3.10/site-packages/autogen/oai/completion.py:224\u001b[0m, in \u001b[0;36mCompletion._get_response\u001b[0;34m(cls, config, raise_on_ratelimit_or_timeout, use_cache)\u001b[0m\n\u001b[1;32m    222\u001b[0m         response \u001b[39m=\u001b[39m openai_completion\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig)\n\u001b[1;32m    223\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 224\u001b[0m         response \u001b[39m=\u001b[39m openai_completion\u001b[39m.\u001b[39;49mcreate(request_timeout\u001b[39m=\u001b[39;49mrequest_timeout, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mconfig)\n\u001b[1;32m    225\u001b[0m \u001b[39mexcept\u001b[39;00m (\n\u001b[1;32m    226\u001b[0m     ServiceUnavailableError,\n\u001b[1;32m    227\u001b[0m     APIConnectionError,\n\u001b[1;32m    228\u001b[0m ):\n\u001b[1;32m    229\u001b[0m     \u001b[39m# transient error\u001b[39;00m\n\u001b[1;32m    230\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mretrying in \u001b[39m\u001b[39m{\u001b[39;00mretry_wait_time\u001b[39m}\u001b[39;00m\u001b[39m seconds...\u001b[39m\u001b[39m\"\u001b[39m, exc_info\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pyautogen/lib/python3.10/site-packages/openai/api_resources/chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[0;32m~/anaconda3/envs/pyautogen/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py:155\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    130\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    131\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    139\u001b[0m ):\n\u001b[1;32m    140\u001b[0m     (\n\u001b[1;32m    141\u001b[0m         deployment_id,\n\u001b[1;32m    142\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    152\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m    153\u001b[0m     )\n\u001b[0;32m--> 155\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    156\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    157\u001b[0m         url,\n\u001b[1;32m    158\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    159\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    160\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    161\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    162\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    163\u001b[0m     )\n\u001b[1;32m    165\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    166\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[1;32m    167\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m~/anaconda3/envs/pyautogen/lib/python3.10/site-packages/openai/api_requestor.py:289\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    279\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    280\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    287\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    288\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m--> 289\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest_raw(\n\u001b[1;32m    290\u001b[0m         method\u001b[39m.\u001b[39;49mlower(),\n\u001b[1;32m    291\u001b[0m         url,\n\u001b[1;32m    292\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    293\u001b[0m         supplied_headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    294\u001b[0m         files\u001b[39m=\u001b[39;49mfiles,\n\u001b[1;32m    295\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    296\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    297\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    298\u001b[0m     )\n\u001b[1;32m    299\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response(result, stream)\n\u001b[1;32m    300\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[0;32m~/anaconda3/envs/pyautogen/lib/python3.10/site-packages/openai/api_requestor.py:606\u001b[0m, in \u001b[0;36mAPIRequestor.request_raw\u001b[0;34m(self, method, url, params, supplied_headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    604\u001b[0m     _thread_context\u001b[39m.\u001b[39msession_create_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m    605\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 606\u001b[0m     result \u001b[39m=\u001b[39m _thread_context\u001b[39m.\u001b[39;49msession\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    607\u001b[0m         method,\n\u001b[1;32m    608\u001b[0m         abs_url,\n\u001b[1;32m    609\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    610\u001b[0m         data\u001b[39m=\u001b[39;49mdata,\n\u001b[1;32m    611\u001b[0m         files\u001b[39m=\u001b[39;49mfiles,\n\u001b[1;32m    612\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    613\u001b[0m         timeout\u001b[39m=\u001b[39;49mrequest_timeout \u001b[39mif\u001b[39;49;00m request_timeout \u001b[39melse\u001b[39;49;00m TIMEOUT_SECS,\n\u001b[1;32m    614\u001b[0m         proxies\u001b[39m=\u001b[39;49m_thread_context\u001b[39m.\u001b[39;49msession\u001b[39m.\u001b[39;49mproxies,\n\u001b[1;32m    615\u001b[0m     )\n\u001b[1;32m    616\u001b[0m \u001b[39mexcept\u001b[39;00m requests\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mTimeout \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    617\u001b[0m     \u001b[39mraise\u001b[39;00m error\u001b[39m.\u001b[39mTimeout(\u001b[39m\"\u001b[39m\u001b[39mRequest timed out: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(e)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pyautogen/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/anaconda3/envs/pyautogen/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    705\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[0;32m~/anaconda3/envs/pyautogen/lib/python3.10/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    483\u001b[0m     timeout \u001b[39m=\u001b[39m TimeoutSauce(connect\u001b[39m=\u001b[39mtimeout, read\u001b[39m=\u001b[39mtimeout)\n\u001b[1;32m    485\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    487\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    488\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    489\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    490\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    491\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    492\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    493\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    494\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    495\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    496\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    497\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    498\u001b[0m     )\n\u001b[1;32m    500\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    501\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(err, request\u001b[39m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/anaconda3/envs/pyautogen/lib/python3.10/site-packages/urllib3/connectionpool.py:715\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    712\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    714\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 715\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    716\u001b[0m     conn,\n\u001b[1;32m    717\u001b[0m     method,\n\u001b[1;32m    718\u001b[0m     url,\n\u001b[1;32m    719\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    720\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    721\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    722\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    723\u001b[0m )\n\u001b[1;32m    725\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    726\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    728\u001b[0m \u001b[39m# mess.\u001b[39;00m\n\u001b[1;32m    729\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pyautogen/lib/python3.10/site-packages/urllib3/connectionpool.py:467\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    462\u001b[0m             httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39mgetresponse()\n\u001b[1;32m    463\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    464\u001b[0m             \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    465\u001b[0m             \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    466\u001b[0m             \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 467\u001b[0m             six\u001b[39m.\u001b[39;49mraise_from(e, \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    468\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    469\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/pyautogen/lib/python3.10/site-packages/urllib3/connectionpool.py:462\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    460\u001b[0m     \u001b[39m# Python 3\u001b[39;00m\n\u001b[1;32m    461\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 462\u001b[0m         httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[1;32m    463\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    464\u001b[0m         \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    465\u001b[0m         \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    466\u001b[0m         \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m    467\u001b[0m         six\u001b[39m.\u001b[39mraise_from(e, \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pyautogen/lib/python3.10/http/client.py:1375\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1374\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1375\u001b[0m         response\u001b[39m.\u001b[39;49mbegin()\n\u001b[1;32m   1376\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n\u001b[1;32m   1377\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/envs/pyautogen/lib/python3.10/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[39m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pyautogen/lib/python3.10/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadline(_MAXLINE \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[39mraise\u001b[39;00m LineTooLong(\u001b[39m\"\u001b[39m\u001b[39mstatus line\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pyautogen/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    706\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pyautogen/lib/python3.10/ssl.py:1307\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1303\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1304\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1305\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1306\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1307\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[1;32m   1308\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1309\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/anaconda3/envs/pyautogen/lib/python3.10/ssl.py:1163\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1161\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1162\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1163\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[1;32m   1164\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1165\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "config, analysis = autogen.Completion.tune(\n",
    "    data=tune_data,  # the data for tuning\n",
    "    metric=\"success\",  # the metric to optimize\n",
    "    mode=\"max\",  # the optimization mode\n",
    "    eval_func=eval_with_generated_assertions,  # the evaluation function to return the success metrics\n",
    "    # log_file_name=\"logs/humaneval.log\",  # the log file name\n",
    "    inference_budget=0.05,  # the inference budget (dollar per instance)\n",
    "    optimization_budget=1,  # the optimization budget (dollar in total)\n",
    "    # num_samples can further limit the number of trials for different hyperparameter configurations;\n",
    "    # -1 means decided by the optimization budget only\n",
    "    num_samples=-1,\n",
    "    prompt=[\n",
    "        \"{definition}\",\n",
    "        \"# Python 3{definition}\",\n",
    "        \"Complete the following Python function:{definition}\",\n",
    "    ],  # the prompt templates to choose from\n",
    "    stop=[[\"\\nclass\", \"\\ndef\", \"\\nif\", \"\\nprint\"], None],  # the stop sequences\n",
    "    config_list=endpoint_list,  # optional: a list of endpoints to use\n",
    "    allow_format_str_template=True,  # whether to allow format string template\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimized config {'prompt': '# Python 3{definition}', 'stop': ['\\nclass', '\\ndef', '\\nif', '\\nprint'], 'allow_format_str_template': True, 'model': 'text-davinci-003', 'max_tokens': 148, 'n': 27, 'top_p': 0.755486898036596}\n",
      "best result on tuning data {'index_selected': 26.0, 'succeed_assertions': 0.0, 'success': 0.5, 'gen_cost': 0.002910000000000001, 'assertions': \"Without an actual function definition and docstring, it's impossible for me to write related assertions. Please provide the necessary information.\", 'total_cost': 0.8951251999999998, 'cost': 0.8645799999999999, 'inference_cost': 0.042286000000000004, 'training_iteration': 0, 'config': {'prompt': 1, 'stop': 0, 'allow_format_str_template': True, 'subspace': {'model': 'text-davinci-003', 'max_tokens': 148, 'temperature_or_top_p': {'top_p': 0.755486898036596}, 'n': 27}}, 'config/prompt': 1, 'config/stop': 0, 'config/allow_format_str_template': True, 'config/subspace': {'model': 'text-davinci-003', 'max_tokens': 148, 'temperature_or_top_p': {'top_p': 0.755486898036596}, 'n': 27}, 'experiment_tag': 'exp', 'time_total_s': 79.82782435417175}\n"
     ]
    }
   ],
   "source": [
    "print(\"optimized config\", config)\n",
    "print(\"best result on tuning data\", analysis.best_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-8RzM4MDLIJgu7h4gGimsNzDZ0ez7r\",\n",
      "  \"warning\": \"This model version is deprecated. Migrate before January 4, 2024 to avoid disruption of service. Learn more https://platform.openai.com/docs/deprecations\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1701681396,\n",
      "  \"model\": \"text-davinci-003\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"    result = []\\n    for i in range(len(game)):\\n        result.append(abs(game[i] - guess[i]))\\n    return result\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"    results = []\\n    for i in range(len(game)):\\n        if game[i] == guess[i]:\\n            results.append(0)\\n        else:\\n            results.append(abs(game[i] - guess[i]))\\n    return results\",\n",
      "      \"index\": 1,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"    results = []\\n    for i in range(len(game)):\\n        if game[i] == guess[i]:\\n            results.append(0)\\n        else:\\n            results.append(abs(game[i]-guess[i]))\\n    return results\",\n",
      "      \"index\": 2,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"    # define an empty list\\n    result = []\\n    # loop through the two lists\\n    for i in range(len(game)):\\n        # check if the values in the two lists are equal\\n        if game[i] == guess[i]:\\n            # if they are equal, append 0 to the result list\\n            result.append(0)\\n        else:\\n            # if they are not equal, append the absolute difference between the two values to the result list\\n            result.append(abs(game[i]-guess[i]))\\n    # return the result list\\n    return result\",\n",
      "      \"index\": 3,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"    result = []\\n    for i in range(len(game)):\\n        result.append(abs(game[i] - guess[i]))\\n    return result\",\n",
      "      \"index\": 4,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"    # initialize an empty list to store the results\\n    results = []\\n    # loop through the game and guess arrays\\n    for i in range(len(game)):\\n        # calculate the absolute difference between the guess and the score\\n        diff = abs(game[i] - guess[i])\\n        # append the difference to the results list\\n        results.append(diff)\\n    # return the results list\\n    return results\",\n",
      "      \"index\": 5,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"    #your code here\\n    res = []\\n    for i in range(len(game)):\\n        if game[i] == guess[i]:\\n            res.append(0)\\n        else:\\n            res.append(abs(game[i] - guess[i]))\\n    return res\",\n",
      "      \"index\": 6,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"    result = []\\n    for i in range(len(game)):\\n        diff = abs(game[i] - guess[i])\\n        result.append(diff)\\n    return result\",\n",
      "      \"index\": 7,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"    res = []\\n    for i in range(len(game)):\\n        res.append(abs(game[i] - guess[i]))\\n    return res\",\n",
      "      \"index\": 8,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"    #your code here\\n    result = []\\n    for i in range(len(game)):\\n        if game[i] == guess[i]:\\n            result.append(0)\\n        else:\\n            result.append(abs(game[i] - guess[i]))\\n    return result\",\n",
      "      \"index\": 9,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"    result = []\\n    for i in range(len(game)):\\n        result.append(abs(game[i] - guess[i]))\\n    return result\",\n",
      "      \"index\": 10,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"    result = []\\n    for i in range(len(game)):\\n        if game[i] == guess[i]:\\n            result.append(0)\\n        else:\\n            result.append(abs(game[i] - guess[i]))\\n    return result\",\n",
      "      \"index\": 11,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"    result = []\\n    for i in range(len(game)):\\n        result.append(abs(game[i]-guess[i]))\\n    return result\",\n",
      "      \"index\": 12,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"    result = []\\n    for i in range(len(game)):\\n        diff = abs(game[i] - guess[i])\\n        result.append(diff)\\n    return result\",\n",
      "      \"index\": 13,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"    result = []\\n    for i in range(len(game)):\\n        result.append(abs(game[i] - guess[i]))\\n    return result\",\n",
      "      \"index\": 14,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"    results = []\\n    for i in range(len(game)):\\n        diff = abs(game[i] - guess[i])\\n        results.append(diff)\\n    return results\",\n",
      "      \"index\": 15,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"    result = []\\n    for i in range(len(game)):\\n        result.append(abs(game[i]-guess[i]))\\n    return result\",\n",
      "      \"index\": 16,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"    # create an empty list\\n    result = []\\n    \\n    # loop over the game and guess\\n    for i in range(len(game)):\\n        # get the difference between the game and guess\\n        diff = abs(game[i] - guess[i])\\n        # append the difference to the result list\\n        result.append(diff)\\n    \\n    # return the result\\n    return result\",\n",
      "      \"index\": 17,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"    result = []\\n    for i in range(len(game)):\\n        diff = abs(game[i] - guess[i])\\n        result.append(diff)\\n    return result\",\n",
      "      \"index\": 18,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"    result = []\\n    for i in range(len(game)):\\n        diff = abs(game[i] - guess[i])\\n        result.append(diff)\\n    return result\",\n",
      "      \"index\": 19,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"    result = []\\n    for i in range(len(game)):\\n        diff = abs(game[i] - guess[i])\\n        result.append(diff)\\n    return result\",\n",
      "      \"index\": 20,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"    res = []\\n    for i in range(len(game)):\\n        diff = abs(game[i] - guess[i])\\n        res.append(diff)\\n    return res\",\n",
      "      \"index\": 21,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"    result = []\\n    for i in range(len(game)):\\n        result.append(abs(game[i] - guess[i]))\\n    return result\",\n",
      "      \"index\": 22,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"    # create an empty list to store the result\\n    result = []\\n    \\n    # loop through the arrays\\n    for i in range(len(game)):\\n        # calculate the absolute difference between the game and guess\\n        diff = abs(game[i] - guess[i])\\n        # append the difference to the result list\\n        result.append(diff)\\n        \\n    return result\",\n",
      "      \"index\": 23,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"    result = []\\n    for i in range(len(game)):\\n        result.append(abs(game[i] - guess[i]))\\n    return result\",\n",
      "      \"index\": 24,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"    result = []\\n    for i in range(len(game)):\\n        if game[i] == guess[i]:\\n            result.append(0)\\n        else:\\n            result.append(abs(game[i] - guess[i]))\\n    return result\",\n",
      "      \"index\": 25,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"    results = []\\n    for i in range(len(game)):\\n        if game[i] == guess[i]:\\n            results.append(0)\\n        else:\\n            results.append(abs(game[i] - guess[i]))\\n    return results\",\n",
      "      \"index\": 26,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 243,\n",
      "    \"completion_tokens\": 1492,\n",
      "    \"total_tokens\": 1735\n",
      "  },\n",
      "  \"cost\": 0.0347,\n",
      "  \"config_id\": 0,\n",
      "  \"pass_filter\": true\n",
      "}\n",
      "{'index_selected': 26, 'succeed_assertions': False, 'success': True, 'gen_cost': 0.0029100000000000003, 'assertions': \"Without an actual function definition and docstring, it's impossible for me to write related assertions. Please provide the necessary information.\"}\n"
     ]
    }
   ],
   "source": [
    "response = autogen.Completion.create(context=tune_data[1], config_list=endpoint_list, **config)\n",
    "print(response)\n",
    "print(eval_with_generated_assertions(autogen.Completion.extract_text(response), **tune_data[1]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyautogen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
